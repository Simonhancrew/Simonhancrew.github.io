---
title: YOLOv4
date: 2021-10-17 14:10:00 +0800
categories: [Blogging]
tags: [writing]
---

v4其实在intro里就说了，很多的tricks都claims可以提点，但是大量的实验需要去验证这个问题。有些功能只对特定的模型或者特定的小规模数据集work。而另外一些tricks，比如BN，resdual connect适用于大多数的模型，任务和数据集。

其实不如把本文看作近些年OD的一个总结，验证了一些方法让OD的性能和速度提升到一个新的stage

然后在看网络结构的时候需要看到具体的细节可以考虑[netron]()

简单的总结以下就是：

1. **输入端：**这里指的创新主要是训练时对输入端的改进，主要包括**Mosaic数据增强、cmBN、SAT自对抗训练**
2. **BackBone主干网络：**将各种新的方式结合起来，包括：**CSPDarknet53、Mish激活函数、Dropblock**
3. **Neck：**目标检测网络在BackBone和最后的输出层之间往往会插入一些层，比如Yolov4中的**SPP模块**、**FPN+PAN结构**
4. **Prediction：**输出层的锚框机制和Yolov3相同，主要改进的是训练时的损失函数**CIOU_Loss**，以及预测框筛选的nms变为**DIOU_nms**

### how it works

#### 输入端数据增强

首先的输入端，数据增强的部分用的Mosaic数据增强（最主要的），**Mosaic数据增强**采用了4张图片，**随机缩放、随机裁剪、随机排布**的方式进行拼接。

原因：小目标的分布不均，增强了一部分的数据分布，另外一次可以计算四张图片的数据，减少了minibatch

#### Backbone

其次就是backbone的部分了，在darknet53的基础上改的，主要就是CSP模块的考虑，每个csp模块都是一个3 * 3的卷积，stride=2，起到一个下采样的作用，有5个CSP块，经过5次下采样得到一个特征图

CSPnet主要解决的问题就是推理的时候计算量过大的问题，推理计算过高的问题是由于网络优化中的**梯度信息重复**导致的。

因此采用CSP模块先将基础层的特征映射划分为两部分，然后通过跨阶段层次结构将它们合并，在减少了计算量的同时可以保证准确率。

这样计算的瓶颈得到了进一步的降低

之后就是mish激活函数的使用了，但是只在backbone里面使用了这种激活函数

第三就是DropBlock的使用，不同于之前的dropout，**Dropblock的研究者**认为，卷积层对于这种随机丢弃并不敏感，因为卷积层通常是三层连用：**卷积+激活+池化层**，池化层本身就是对相邻单元起作用。而且即使随机丢弃，卷积层仍然可以从相邻的激活单元学习到**相同的信息**，不如整个局部区域进行删减丢弃。

#### Neck

更好的提取融合特征，在backbone和输出之间会加一些层，这个部分就是neck,yolo主要用的就是SPP，FPN + PAN

首先，SPP模块，使用k={1*1,5*5,9*9,13*13}的最大池化的方式，再将不同尺度的特征图进行Concat操作。

最大池化采用**padding操作**，移动的步长为1，比如13×13的输入特征图，使用5×5大小的池化核池化，**padding=2**，因此池化后的特征图仍然是13×13大小。

我的理解是深度很重要，宽度也很重要，更有效的增加主干特征的接收范围，显著的分离了最重要的上下文特征。

其次，PAN + SPN借鉴的就是18年的PANet

#### prediction的创新

cls loss和bbox reg loss组成了全部的loss，bbox的loss这些年的发展比较好了

从RCNN系列的smooth l1 loss,到IoU los，再到GIoU loss，DIoU loss,最后的CIoU loss。

Yolo中的bbox reg采用了CIoU loss，然后nms中计算IoU的部分换成了DIoU。

